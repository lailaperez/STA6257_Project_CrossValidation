---
title: "Cross-Validation Capstone Project"
author: "Laila Perez, Sai Devarashetty, Andrew Mattick, Curtis Musson"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

CV (Cross-Validation) is one of the most important methods for evaluating the performance and ability of a model to work in various domains, ranging from psychology to mobile health, software engineering, ecology, and climatology. CV involves dividing the data into training and testing portions, which strongly supports model validation to minimize overfitting. Song, Tang, and Wee show how to implement CV in the context of R and Shiny for psychological research and underscore the criticality of overfitting prevention, especially with small samples and high dimensions [@song2021making]. Their study shows that repeated k-fold CV offers better results in sample sizes less than 500 and helps improve models' generalizability and robustness [@song2021making].

Negative binomial regression is compared with binary logistic regression by Yu in software engineering to model faults of Apache Ant modules [@yu2012using]. Although negative binomial regression is lacking in some ways, it is superior in predicting the number of bugs per module. Thus, it is important to select the right regression models and use CV to confirm the accuracy of the models. In the present work, Allgaier and Pryss specifically focus on CV techniques in mobile health research and discuss some train-test-split approaches, namely, stratified and nested k-fold CV [@allgaier2024practical]. They explain that covert populations may distort the outcomes and show that CV is useful in improving model accuracy and diminishing the impact of bias by appealing to analysts' expertise [@allgaier2024practical].

Depending on the research, data, and field of study, researchers may prefer different methodologies. In the book by Gareth James, "An Introduction to Statistical Learning", three kinds of CV are discussed: the Validation Set Approach, Leave-One-Out, and k-fold [@james2013introduction]. After discussing these methods, the author concludes that k-fold (where k=5 or 10) is the preferred method because the required computations are not very high, and it doesn’t show excessive bias or high variance [@james2013introduction]. These results were similar to those from the study by Wong and Yeh, who tested the accuracy estimates of k-folds with increasing k values to k=1 [@wong2019reliable]. They tested the methods against 20 different datasets and noted the correlation between the number of folds and the accuracy of the model. They ultimately concluded that using a larger number of folds with fewer repetitions is recommended for evaluating classification algorithms to get more reliable accuracy estimates [@wong2019reliable].

In Bischl’s article studying resampling methods, he concludes that LOO (Leave-One-Out) has too high a variance and chooses overly complex methods [@bischl2012resampling]. Yates et al. provide an overview of CV methods in ecological modeling; they recommend using LOO and SDS (Stratified Double Sampling) due to their efficiency in reducing bias and overfitting of the data [@yates2023cross]. They also stress the aspects of model selection according to the study's aim: hypothesis testing, prediction, and causality, which provides useful information about the use of CV in ecological research [@yates2023cross]. Vanwinckelen and Blockeel discuss the use of repeated CV concerning the evaluation of model accuracy [@vanwinckelen2012estimating]. They provide evidence that shows CV has a pessimistic bias, which increases with the number of CV repetitions [@vanwinckelen2012estimating].

Hawkins, Basak, and Mills discuss the Validation Set Approach and compare it with CV in the development of QSAR models [@hawkins2003assessing]. They provide evidence that CV offers more accurate performance estimates compared to the Validation Set Approach, especially with small samples – a crucial aspect in developing good and accurate QSAR models in chemical and pharmaceutical research [@hawkins2003assessing]. Mnich et al. delve into the application of repeated CV in super learning to lower high variance and provide better performance estimates, increasing predictive capacity and decreasing computation time [@mnich2020super].

According to Yates, there are four goals when selecting a model: exploration, hypothesis testing, prediction, and causal inference [@yates2023cross]. While a researcher may prefer one method to meet all four goals, these goals are often mutually exclusive. It is up to the researcher to determine which goal is most important and which method best meets those goals. Tougui et al. explore the effect of different CV strategies on the performance of diagnostic applications derived using machine learning [@tougui2021impact]. They found that while record-wise CV tends to provide optimistic estimates of classifiers' performance, subject-wise CV gives 'real-world' measures of performance. This is further substantiated by the fact that climate and weather prediction models also employ cross-validations, reflecting the appropriateness and flexibility of these techniques. Effective models and forecasts regarding weather help shape climate policy in this field.

When choosing the method, it is important to note the limitations of the method, the data, and the computation time. Browne points out that splitting the data limits the number of observations and notes that accuracy depends on the index of the predictors and the sample size [@browne2000cross]. One of the biggest dangers in machine learning is overfitting, where the model created from the training data is a great predictor of the training data but overestimates the results for data not found in the training and test data. In the research by Allgaier, they found that the predictions were skewed due to hidden groups in the data [@allgaier2024practical]. These groups were not recognizable by machine learning and required a health field expert to identify that people who made multiple entries had more effect on the prediction than those who made fewer entries. Once they recognized the error, they switched to stratified cross-validation to mitigate the effects [@allgaier2024practical].

Other methods have been developed to address overfitting. The method of Cross Validation with Confidence (CVC) was proposed in an article by Jing Lei [@lei2020cross]. The goal is to test each model against the null hypothesis that it is the best model, providing a p-value for each model based on cross-validation residuals and a Gaussian multiplier bootstrap approach [@lei2020cross]. This method balances prediction accuracy and model interpretability but increases computational complexity by adding p-values [@lei2020cross]. Filzmoser et al. proposed repeated double cross-validation (rdCV), involving partial least squares regression and repeated double cross-validation [@filzmoser2009repeated]. It splits the dataset multiple times to check performance. The limitations of this method are its computational intensity for large datasets and the possibility of different results in some cases due to random data splits [@filzmoser2009repeated].



## Methods

##### Square Error

$$
Square\ Error_i = (y_i - \hat{y}_i)^2\tag{3}
$$

##### General Linear Model

$$
y = \beta_0 + \beta_1 X_1 + ... +\beta_k X_k +\varepsilon_i\tag{3}
$$

##### Mean Square Error

$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i -\hat{y}_i)^2  \tag{3}
$$


##### Leave-One_Out Cross validation

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} MSE_i  \tag{3}
$$


#### leverage statistic

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\tag{3}
$$


$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_i - \hat{y}}{1-h_i}\right)^2\tag{3}
$$



#### k-Fold Cross-Validation


$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} MSEi
$$



## Analysis and Results

### Data and Visualization

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```


### Data and Visualization

concrete data [@misc_concrete_compressive_strength_165]

| Name                          | Data Type    | Measurement  | Variable     |
|:----------------------------- |:-------------|:-------------|:----------------|
| Concrete compressive strength | Quantitative | MPa          | Response  |
| Cement              | Quantitative | kg in a m3 mixture | Predictor |
| Blast Furnace Slag  | Quantitative | kg in a m3 mixture | Predictor |
| Fly Ash             | Quantitative | kg in a m3 mixture | Predictor |
| Water               | Quantitative | kg in a m3 mixture | Predictor |
| Superplasticizer    | Quantitative | kg in a m3 mixture | Predictor |
| Coarse Aggregate    | Quantitative | kg in a m3 mixture | Predictor |
| Fine Aggregate      | Quantitative | kg in a m3 mixture | Predictor |
| Age                 | Quantitative | Day (1~365)        | Predictor |

: **Variables** {.striped .hover}

### Extract transform and Load Data

```{r, warning=FALSE, echo=TRUE}
library(dplyr)
library(readxl)

# Load Data
concrete <- read_excel("Concrete_Data.xls") %>%
  rename(cement = 1,
         furnac_slag = 2,
         fly_ash = 3,
         water = 4,
         superplasticizer = 5,
         coarse_aggregate = 6,
         fine_aggregate = 7,
         age = 8,
         compressive_strength = 9 ) %>%
  relocate(compressive_strength)

```

### Data Plots

```{r}
library(ggplot2)
library(stringr)
library(tidyr)

pivot_longer(data = concrete, 1:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x=factor(0), y = value)) +
  geom_boxplot(color="blue", outlier.color="red") +
  xlab("Predictor") + ylab("") +
  theme(axis.text.x = element_blank()) +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  theme(strip.text = element_text(face = "bold"))

pivot_longer(data = concrete, 2:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x = value, y = compressive_strength)) +
  geom_point() +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  xlab("Predictor") + 
  ylab("Compressive Strength") +
  theme(strip.text = element_text(face = "bold")) +
  geom_smooth(formula = y ~ x, method = lm, se = FALSE)
```


### Statistical Modeling

### Conclusion

## References
