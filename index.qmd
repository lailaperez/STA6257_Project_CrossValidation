---
title: "Cross-Validation Capstone Project"
author: "Laila Perez, Sai Devarashetty, Andrew Mattick, Curtis Musson"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

CV (Cross-Validation) is one of the most important methods for evaluating the performance and ability of a model to work in various domains, ranging from psychology to mobile health, software engineering, ecology, and climatology. CV involves dividing the data into training and testing portions, which strongly supports model validation to minimize overfitting. Song, Tang, and Wee show how to implement CV in the context of R and Shiny for psychological research and underscore the criticality of overfitting prevention, especially with small samples and high dimensions [@song2021making]. Their study shows that repeated k-fold CV offers better results in sample sizes less than 500 and helps improve models' generalizability and robustness [@song2021making].

Negative binomial regression is compared with binary logistic regression by Yu in software engineering to model faults of Apache Ant modules [@yu2012using]. Although negative binomial regression is lacking in some ways, it is superior in predicting the number of bugs per module. Thus, it is important to select the right regression models and use CV to confirm the accuracy of the models. In the present work, Allgaier and Pryss specifically focus on CV techniques in mobile health research and discuss some train-test-split approaches, namely, stratified and nested k-fold CV [@allgaier2024practical]. They explain that covert populations may distort the outcomes and show that CV is useful in improving model accuracy and diminishing the impact of bias by appealing to analysts' expertise [@allgaier2024practical].

Depending on the research, data, and field of study, researchers may prefer different methodologies. In the book by Gareth James, "An Introduction to Statistical Learning", three kinds of CV are discussed: the Validation Set Approach, Leave-One-Out, and k-fold [@james2013introduction]. After discussing these methods, the author concludes that k-fold (where k=5 or 10) is the preferred method because the required computations are not very high, and it doesn’t show excessive bias or high variance [@james2013introduction]. These results were similar to those from the study by Wong and Yeh, who tested the accuracy estimates of k-folds with increasing k values to k=1 [@wong2019reliable]. They tested the methods against 20 different datasets and noted the correlation between the number of folds and the accuracy of the model. They ultimately concluded that using a larger number of folds with fewer repetitions is recommended for evaluating classification algorithms to get more reliable accuracy estimates [@wong2019reliable].

In Bischl’s article studying resampling methods, he concludes that LOO (Leave-One-Out) has too high a variance and chooses overly complex methods [@bischl2012resampling]. Yates et al. provide an overview of CV methods in ecological modeling; they recommend using LOO and SDS (Stratified Double Sampling) due to their efficiency in reducing bias and overfitting of the data [@yates2023cross]. They also stress the aspects of model selection according to the study's aim: hypothesis testing, prediction, and causality, which provides useful information about the use of CV in ecological research [@yates2023cross]. Vanwinckelen and Blockeel discuss the use of repeated CV concerning the evaluation of model accuracy [@vanwinckelen2012estimating]. They provide evidence that shows CV has a pessimistic bias, which increases with the number of CV repetitions [@vanwinckelen2012estimating].

Hawkins, Basak, and Mills discuss the Validation Set Approach and compare it with CV in the development of QSAR models [@hawkins2003assessing]. They provide evidence that CV offers more accurate performance estimates compared to the Validation Set Approach, especially with small samples – a crucial aspect in developing good and accurate QSAR models in chemical and pharmaceutical research [@hawkins2003assessing]. Mnich et al. delve into the application of repeated CV in super learning to lower high variance and provide better performance estimates, increasing predictive capacity and decreasing computation time [@mnich2020super].

According to Yates, there are four goals when selecting a model: exploration, hypothesis testing, prediction, and causal inference [@yates2023cross]. While a researcher may prefer one method to meet all four goals, these goals are often mutually exclusive. It is up to the researcher to determine which goal is most important and which method best meets those goals. Tougui et al. explore the effect of different CV strategies on the performance of diagnostic applications derived using machine learning [@tougui2021impact]. They found that while record-wise CV tends to provide optimistic estimates of classifiers' performance, subject-wise CV gives 'real-world' measures of performance. This is further substantiated by the fact that climate and weather prediction models also employ cross-validations, reflecting the appropriateness and flexibility of these techniques. Effective models and forecasts regarding weather help shape climate policy in this field.

When choosing the method, it is important to note the limitations of the method, the data, and the computation time. Browne points out that splitting the data limits the number of observations and notes that accuracy depends on the index of the predictors and the sample size [@browne2000cross]. One of the biggest dangers in machine learning is overfitting, where the model created from the training data is a great predictor of the training data but overestimates the results for data not found in the training and test data. In the research by Allgaier, they found that the predictions were skewed due to hidden groups in the data [@allgaier2024practical]. These groups were not recognizable by machine learning and required a health field expert to identify that people who made multiple entries had more effect on the prediction than those who made fewer entries. Once they recognized the error, they switched to stratified cross-validation to mitigate the effects [@allgaier2024practical].

Other methods have been developed to address overfitting. The method of Cross Validation with Confidence (CVC) was proposed in an article by Jing Lei [@lei2020cross]. The goal is to test each model against the null hypothesis that it is the best model, providing a p-value for each model based on cross-validation residuals and a Gaussian multiplier bootstrap approach [@lei2020cross]. This method balances prediction accuracy and model interpretability but increases computational complexity by adding p-values [@lei2020cross]. Filzmoser et al. proposed repeated double cross-validation (rdCV), involving partial least squares regression and repeated double cross-validation [@filzmoser2009repeated]. It splits the dataset multiple times to check performance. The limitations of this method are its computational intensity for large datasets and the possibility of different results in some cases due to random data splits [@filzmoser2009repeated].



## Methods

### Model Measures of Error

Measuring the quality of fit of a regression model is an important step in data modeling. There are several commonly used metrics that quantify how well a model explains the data. By measuring the quality of fit we can select the model that makes the most accurate predictions on unseen data. Common metrics used to measure model performance are:

- **Mean Absolute Error (MAE)**\
\
The Mean Absolute Error is a measure error <u>magnitude</u>. The sine of the error does not matter because MAE uses the absolute value. Small MAE values, "<i>lower magnitude</i>" indicate better model fit. MAE is calculated (1) by averaging the absolute difference between the observed $(y_i)$ and predicted $\hat{f}(x_i)$ values. Where:\
\
  - $n$ is the number of observations,
  - $\hat{f}(x_i)$ is the prediction that the regression function $\hat{f}$ gives for the ith observation,
  - $y_i$ is the observed value.
  
$$
 \text{MAE} 
 = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{f}(x_i)|  
 \tag{1}
$$
\

- **Root Mean Squared Error (RMSE)**\
\
The Root Mean Squared Error (2) is a measure of error <u>magnitude</u> also. Like MAE, smaller RMSE values indicate better model fit.
In this method the square error $(y_i - \hat{f}(x_i))^2$  values are used. Squaring the error give more weight to the larger ones. In contrast with the MAE that uses the absolute error $|y_i - \hat{f}(x_i)|$ values, all errors are weighted equally regardless of size. Taking the square root returns the error to the same units as the response variable, making it easier to interpret.\

$$
\text{RMSE} 
= \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2}
\tag{2}
$$

\

- **R-squared ($R^2$)**\
\
R-squared is the proportion, <i>percent</i> of the variance in the response variable that is explained by the predictor variable(s). Unlike MAR and RMSE, $R^2$ values range from 0 to 1 and the higher the value, the better the fit. An $R^2$ value of 0.75 indicates that 75% of the variance in the response variable can be explained by the predictor variable(s). The $R^2$ equation (3) is composed of two key parts, the Total Sum of Squares ($SS_{tot}$) and the Residual Sum of Squares ($SS_{res}$).

$$
\text{R}^2 
= \frac{SS_{tot}-SS_{res}}{SS_{tot}} 
= 1 - \frac{SS_{res}}{SS_{tot}}
= 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}{\sum_{i=1}^{n}(y_i-\bar{f}(x_i))^2}
 \tag{3}
$$ 


  [@james2013introduction]
 
### Leave One Out Cross-validations (LOOCV)

A specific case of K-Fold Cross-Validation, where k is the number of data points in the dataset, is Leave-One-Out Cross-Validation (LOOCV). With this approach, the model is trained N times, with the remaining N−1 observations being used for training and one observation being left out as the test set each time. Even with its high computing cost, LOOCV offers an objective assessment of the model's efficacy. Lei talks about how LOOCV is good at giving a thorough evaluation of model performance, particularly for small datasets [@lei2020cross]. However, as Browne points out, a drawback may be the huge variance in error estimation brought on by the frequent usage of almost identical training sets [@browne2000cross].

![Leave One Out CV](LOOCV_fig.webp){#fig-LOOCV}

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \text{Measuer of Errori}_i \tag{4}
$$

### k-Fold Cross-Validation

K-Fold Cross-Validation is a widely used technique for assessing the performance of a predictive model. The dataset is divided using this method into k roughly equal-sized folds. After k−1folds of training, the model is tested on the last fold. Every fold is used as the test set once, and this process is repeated k times. Next, an average of the outcomes is obtained to generate a single performance estimate. Wong and Yeh state that this approach is a strong option for model validation since it yields accurate estimates and is particularly useful for smaller datasets for which separate training and testing sets are impractical [@wong2019reliable]. Additionally, Browne highlights the significance of K-Fold CV in empirical model evaluation and elaborates on the flexibility and adaptability of the model in handling various data distributions and sizes [@browne2000cross]. 

![5-fold CV](CV5Fold_fig.webp){#fig-kfold}

$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} \text{Measuer of Errori}_i \tag{5}
$$

### Nested Cross-Validation

Nested Cross-Validation is especially helpful when choosing a model and adjusting hyperparameters. Two stages of cross-validation are used in this technique: an inner loop that optimizes hyperparameters and an outside loop that assesses the model's performance. Nested cross-validation offers a more objective estimation of the model's capacity for generalization by isolating the hyperparameter tweaking procedure from the performance assessment. According to Filzmoser and his colleagues, nested cross-validation is useful for generating precise performance estimates without causing overfitting, which can happen when hyperparameters are adjusted using the same data that is used for model evaluation [@filzmoser2009repeated]. This technique is particularly important in complicated modeling situations when the model's prediction performance is greatly impacted by parameter adjustment.

![Nested CV](NestCV_fig.webp){#fig-NestCV}


##### Square Error

$$
Square\ Error_i = (y_i - \hat{y}_i)^2\tag{3}
$$

##### General Linear Model

$$
y = \beta_0 + \beta_1 X_1 + ... +\beta_k X_k +\varepsilon_i\tag{3}
$$

##### Mean Square Error

$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i -\hat{y}_i)^2  \tag{3}
$$


##### Leave-One_Out Cross validation

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} MSE_i  \tag{3}
$$


#### leverage statistic

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\tag{3}
$$


$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_i - \hat{y}}{1-h_i}\right)^2\tag{3}
$$



#### k-Fold Cross-Validation


$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} MSEi
$$



## Analysis and Results

### Data and Visualization

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```


### Data and Visualization

concrete data [@misc_concrete_compressive_strength_165]

| Name                          | Data Type    | Measurement  | Variable     |
|:----------------------------- |:-------------|:-------------|:----------------|
| Concrete compressive strength | Quantitative | MPa          | Response  |
| Cement              | Quantitative | kg in a m3 mixture | Predictor |
| Blast Furnace Slag  | Quantitative | kg in a m3 mixture | Predictor |
| Fly Ash             | Quantitative | kg in a m3 mixture | Predictor |
| Water               | Quantitative | kg in a m3 mixture | Predictor |
| Superplasticizer    | Quantitative | kg in a m3 mixture | Predictor |
| Coarse Aggregate    | Quantitative | kg in a m3 mixture | Predictor |
| Fine Aggregate      | Quantitative | kg in a m3 mixture | Predictor |
| Age                 | Quantitative | Day (1~365)        | Predictor |

: **Variables** {.striped .hover}

### Extract transform and Load Data

```{r, warning=FALSE, echo=TRUE}
library(dplyr)
library(readxl)

# Load Data
concrete <- read_excel("Concrete_Data.xls") %>%
  rename(cement = 1,
         furnac_slag = 2,
         fly_ash = 3,
         water = 4,
         superplasticizer = 5,
         coarse_aggregate = 6,
         fine_aggregate = 7,
         age = 8,
         compressive_strength = 9 ) %>%
  relocate(compressive_strength)

```

### Data Plots

```{r}
library(ggplot2)
library(stringr)
library(tidyr)

pivot_longer(data = concrete, 1:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x=factor(0), y = value)) +
  geom_boxplot(color="blue", outlier.color="red") +
  xlab("Predictor") + ylab("") +
  theme(axis.text.x = element_blank()) +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  theme(strip.text = element_text(face = "bold"))

pivot_longer(data = concrete, 2:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x = value, y = compressive_strength)) +
  geom_point() +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  xlab("Predictor") + 
  ylab("Compressive Strength") +
  theme(strip.text = element_text(face = "bold")) +
  geom_smooth(formula = y ~ x, method = lm, se = FALSE)
```


### Statistical Modeling

### Conclusion

## References
