---
title: "Cross-Validation Capstone Project"
author: "Laila Perez"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

CV (Cross-Validation) is one of the most important methods for evaluating the performance and ability of a model to work in various domains, ranging from psychology to mobile health, software engineering, ecology, and climatology. CV involves dividing the data into training and testing portions, which strongly supports model validation to minimize overfitting. Song, Tang, and Wee show how to implement CV in the context of R and Shiny for psychological research and underscore the criticality of overfitting prevention, especially with small samples and high dimensions [@song2021making]. Their study shows that repeated k-fold CV offers better results in sample sizes less than 500 and helps improve models' generalizability and robustness [@song2021making].

Negative binomial regression is compared with binary logistic regression by Yu in software engineering to model faults of Apache Ant modules [@yu2012using]. Although negative binomial regression is lacking in some ways, it is superior in predicting the number of bugs per module. Thus, it is important to select the right regression models and use CV to confirm the accuracy of the models. In the present work, Allgaier and Pryss specifically focus on CV techniques in mobile health research and discuss some train-test-split approaches, namely, stratified and nested k-fold CV [@allgaier2024practical]. They explain that covert populations may distort the outcomes and show that CV is useful in improving model accuracy and diminishing the impact of bias by appealing to analysts' expertise [@allgaier2024practical].

Depending on the research, data, and field of study, researchers may prefer different methodologies. In the book by Gareth James, "An Introduction to Statistical Learning", three kinds of CV are discussed: the Validation Set Approach, Leave-One-Out, and k-fold [@james2013introduction]. After discussing these methods, the author concludes that k-fold (where k=5 or 10) is the preferred method because the required computations are not very high, and it doesnâ€™t show excessive bias or high variance [@james2013introduction]. These results were similar to those from the study by Wong and Yeh, who tested the accuracy estimates of k-folds with increasing k values to k=1 [@wong2019reliable]. They tested the methods against 20 different datasets and noted the correlation between the number of folds and the accuracy of the model. They ultimately concluded that using a larger number of folds with fewer repetitions is recommended for evaluating classification algorithms to get more reliable accuracy estimates [@wong2019reliable].

In Bischlâ€™s article studying resampling methods, he concludes that LOO (Leave-One-Out) has too high a variance and chooses overly complex methods [@bischl2012resampling]. Yates et al. provide an overview of CV methods in ecological modeling; they recommend using LOO and SDS (Stratified Double Sampling) due to their efficiency in reducing bias and overfitting of the data [@yates2023cross]. They also stress the aspects of model selection according to the study's aim: hypothesis testing, prediction, and causality, which provides useful information about the use of CV in ecological research [@yates2023cross]. Vanwinckelen and Blockeel discuss the use of repeated CV concerning the evaluation of model accuracy [@vanwinckelen2012estimating]. They provide evidence that shows CV has a pessimistic bias, which increases with the number of CV repetitions [@vanwinckelen2012estimating].

Hawkins, Basak, and Mills discuss the Validation Set Approach and compare it with CV in the development of QSAR models [@hawkins2003assessing]. They provide evidence that CV offers more accurate performance estimates compared to the Validation Set Approach, especially with small samples â€“ a crucial aspect in developing good and accurate QSAR models in chemical and pharmaceutical research [@hawkins2003assessing]. Mnich et al. delve into the application of repeated CV in super learning to lower high variance and provide better performance estimates, increasing predictive capacity and decreasing computation time [@mnich2020super].

According to Yates, there are four goals when selecting a model: exploration, hypothesis testing, prediction, and causal inference [@yates2023cross]. While a researcher may prefer one method to meet all four goals, these goals are often mutually exclusive. It is up to the researcher to determine which goal is most important and which method best meets those goals. Tougui et al. explore the effect of different CV strategies on the performance of diagnostic applications derived using machine learning [@tougui2021impact]. They found that while record-wise CV tends to provide optimistic estimates of classifiers' performance, subject-wise CV gives 'real-world' measures of performance. This is further substantiated by the fact that climate and weather prediction models also employ cross-validations, reflecting the appropriateness and flexibility of these techniques. Effective models and forecasts regarding weather help shape climate policy in this field.

When choosing the method, it is important to note the limitations of the method, the data, and the computation time. Browne points out that splitting the data limits the number of observations and notes that accuracy depends on the index of the predictors and the sample size [@browne2000cross]. One of the biggest dangers in machine learning is overfitting, where the model created from the training data is a great predictor of the training data but overestimates the results for data not found in the training and test data. In the research by Allgaier, they found that the predictions were skewed due to hidden groups in the data [@allgaier2024practical]. These groups were not recognizable by machine learning and required a health field expert to identify that people who made multiple entries had more effect on the prediction than those who made fewer entries. Once they recognized the error, they switched to stratified cross-validation to mitigate the effects [@allgaier2024practical].

Other methods have been developed to address overfitting. The method of Cross Validation with Confidence (CVC) was proposed in an article by Jing Lei [@lei2020cross]. The goal is to test each model against the null hypothesis that it is the best model, providing a p-value for each model based on cross-validation residuals and a Gaussian multiplier bootstrap approach [@lei2020cross]. This method balances prediction accuracy and model interpretability but increases computational complexity by adding p-values [@lei2020cross]. Filzmoser et al. proposed repeated double cross-validation (rdCV), involving partial least squares regression and repeated double cross-validation [@filzmoser2009repeated]. It splits the dataset multiple times to check performance. The limitations of this method are its computational intensity for large datasets and the possibility of different results in some cases due to random data splits [@filzmoser2009repeated].


### Laila summaries

<u>Summary of "Cross-Validation Methods" by Michael Browne</u>

Goal

The goal of this paper was to review cross-validation methods. Browne (2020) aimed to evaluate the use of cross-validation for assessing the predictive validity of models, specifically focusing on structural models for moment matrices.

Methods

The paper discusses different cross-validation techniques. It examines the two-sample and single-sample cross-validation indices. In the first, the data is split into two samples: a calibration sample and a validation sample. The calibration sample is used to develop the model, and the validation sample is used to assess the model's predictive validity. This approach provides a realistic impression of the model's effectiveness but can be wasteful as it uses only part of the data for calibration. In the second (single-sample), it estimates the expected value of the cross-validation index directly from the calibration sample without splitting the data. This approach requires additional distributional assumptions about the data.

Browne (2020) also explores the application of cross-validation to multiple linear regression and extends their use to the analysis of moment structures, which involves using sample-based calibration of models to analyze covariance and other moment structures, comparing different models to determine their appropriateness. Lastly, it also considers the Akaike information criterion as a single-sample cross-validation index. According to Browne (2020), AIC is used to assess the fit of probability models and is related to the single-sample cross-validation index in the context of maximum likelihood estimation.

Results

Browne (2020) found that cross-validation methods can provide realistic assessments of predictive validity. It shows that predictive accuracy depends on the sample size and the number of predictor variables. Single-sample and two-sample cross-validation indices help in determining the optimal number of parameters for a model based on sample size. The study confirms that while cross-validation is useful, it has limitations that I will mention below.

Limitations

When it comes to its limitations, cross-validation can be wasteful as it often requires splitting the data into calibration and validation samples, which reduces the number of observations available for calibration. Browne (2020) also mentions that the accuracy of cross-validation results also depends on the index of predictive accuracy employed. Also, cross-validation is less effective when the sample size is small.

Data Used

Browne (2020) uses theoretical data and mathematical derivations to discuss the principles and effectiveness of cross-validation methods. It also references historical data from previous studies and examples to illustrate the concepts. For example, the paper mentions Mosier's (1951) work on the predictive validity of linear regression equations and various extensions provided by Stone (1974) and Geisser (1975). Browne (2020) also used simulated data; for instance, he includes a sampling experiment using data from a longitudinal study on high school students' ability tests.


<u>Summary of "Cross-Validation with Confidence" by Jing Lei</u>

Goal/Why?

The goal of this article was to address the overfitting problem in traditional cross-validation methods used for model and tuning parameter selection in statistics and machine learning. Lei (2020) proposes a new method called Cross-Validation with Confidence (CVC) that incorporates the uncertainty in the testing sample to provide more reliable model selection.

Methods

Lei (2020) develops a hypothesis testing framework for cross-validation. This framework evaluates each candidate model or tuning parameter by testing the null hypothesis that it has the smallest predictive risk among all candidates. The method involves calculating p-values for each candidate model based on cross-validated residuals and using a Gaussian multiplier bootstrap approach.

Results

The CVC method can achieve consistent variable selection in classical linear regression settings with conventional split ratios. It provides a confidence set of highly competitive models, ensuring that the best model is included with high probability. The method is demonstrated to perform well in both simulated and real data examples, offering a good balance between prediction accuracy and model interpretability.

Limitations

Lei (2020) mentions that this method requires additional computational complexity due to the calculation of p-values and bootstrap sampling. The performance depends on the choice of significance level, and while the method is sound (in theory), its practicality and effectiveness in very high-dimensional settings may require more exploration.

Data used

Lei (2020) uses several simulated datasets to demonstrate the performance of the CVC method. The simulations include both low-dimensional and high-dimensional scenarios, as well as different noise distributions. Also, real data examples are used to illustrate the practical applicability of the method.

References

Browne, M. W. (2000). Cross-Validation Methods. Journal of Mathematical Psychology, 44(1), 108-132. https://doi.org/10.1006/jmps.1999.1279)
Jing Lei (2020). Cross-Validation with Confidence. Journal of the American Statistical Association, 115(532), 1978-1997. https://doi.org/10.1080/01621459.2019.1672556)


<u>Summary for Filzmoser et al. (2009)</u>

Goal/Why?

In their publication, Filzmoser et al. (2009) tried to improve the process of making regression models better. They introduce a method called repeated double cross-validation (rdCV) to make sure these models are accurate and not too complex. This method is useful for small datasets.

Methods

Filzmoser et al. (2009) used a process called partial least-squares regression and combined it with repeated double cross-validation. This involves splitting the data into different sets multiple times to check the model's performance. They also used a tool called a genetic algorithm to choose important variables for the models.

Results

The rdCV method was tested on two datasets: one for predicting glucose concentrations in bioethanol production and another for modeling gas chromatographic retention indices of polycyclic aromatic compounds. The results showed that rdCV helps in finding the best model complexity and provides a reliable estimation of prediction errors.

Limitations

The main limitation in Filzmoser et al.â€™s (2009) study is that the rdCV method can be computationally intensive, especially with large datasets or many repetitions. The method also relies on random splits of data, which might lead to different results in some cases.

Data That Was Used

Data of glucose concentration and polycyclic aromatic compounds were used for this study. Specifically, data from 120 mash samples in bioethanol production, including NIR absorbance spectra and glucose concentrations. Also, data from 209 compounds with their gas chromatographic retention indices and molecular descriptors.


<u>Summary for Wong and Yeh (2020)</u>

Goal/Why?

In their publication, Wong and Yeh (2020) tried to determine whether k-fold cross-validation should be repeated to get reliable accuracy estimates. Wong and Yeh (2020) explored how the dependency among accuracy estimates from different repetitions affects the reliability of these estimates.

Methods

Wong and Yeh (2020) used k-nearest neighbors (k=1) to analyze dependency relationships. They performed k-fold cross-validation multiple times and used statistical methods to test the dependency level between accuracy estimates from different repetitions. They analyzed twenty datasets to understand the correlation between accuracy estimates and the number of folds used.

Results

The results showed that the accuracy estimates from repeated k-fold cross-validation are highly correlated. The correlation increases with the number of folds. Therefore, using a large number of folds with fewer repetitions is recommended for evaluating classification algorithms to get more reliable accuracy estimates.

Limitations

One limitation is the dependency among accuracy estimates, which might affect the variance estimation. Another limitation is that the method's computational cost increases with the number of repetitions and folds.

References

Wong, T.-T., & Yeh, P.-Y. (2020). Reliable Accuracy Estimates from k-Fold Cross Validation. IEEE Transactions on Knowledge and Data Engineering, 32(8), 1586â€“1594. https://doi.org/10.1109/TKDE.2019.2912815
Filzmoser, P., Liebmann, B., & Varmuza, K. (2009). Repeated double cross validation. Journal of Chemometrics, 23(4), 160â€“171. https://doi.org/10.1002/cem.1225


### Andy summaries

Song, Q. Chelsea, Chen Tang, and Serena Wee. "Making sense of model generalizability: A tutorial on cross-validation in R and Shiny." Advances in Methods and Practices in Psychological Science 4.1 (2021): 2515245920947067.
GOAL: Provide a tutorial about how to use cross validation to research. 

METHODS: 
-	They used a Shiny app that tested the ability of people to learn new words and compared that to a personâ€™s performance on a learning task.
-	They took the regression line created and determined if the line overfit the data.
-	They determined how well the data worked on new data by comparing the Mean Squared Error
-	They then looked at 5-fold and 10-fold Cross Validation as a method to deal with the overfitting of data.

RESULTS:
-	They determined that using the regression line, that the results overfit the data.
-	They determined that if the sample size was less than 500, repeated k-fold Cross Validation was better.
-	They determined that cross validation was better in high dimensional data sets.
-	The Cross Validation R squared tells you how much of the results can be explained by the prediction method.  The Cross Validation MSE tells you on average how far each prediction is from the actual values.

LIMITATIONS:
-	Small datasets also limited the value of simple cross validation.
-	Regression overfits the data and reduces the generalizability of the prediction.  .  It was worse when the datasets were smaller.

DATA: The data came from 71,982 participants who completed the online version of the Mach-IV measure of Machiavellianism.  The original data came from https://github.com/qcsong/CrossValidationTutorial.


Using Negative Binomial Regression Analysis to Predict Software Faults: A Study of Apache Ant Liguo Yu Computer Science and Informatics, Indiana University South Bend, South Bend, IN, USA Email: ligyu@iusb.edu

GOAL: To compare the performance of negative binomial regression to binary logistic regression in predicting fault-prone modules.  

METHOD: 
-	They used negative binomial regression to predict the number of faults in every module that was going to be released.
-	They used Spearmanâ€™s rank correlation test to determine if each of the 20 predictors would be a significant predictor
-	They eliminated all characteristics that had negative correlation in any of the 5 versions being tested and made the model based on just the 9.
-	They determined prediction accuracy, prediction precision and recall rate.
-	They determined recall rate to be the most valuable in predicting if a module would have error.

RESULTS:
-	Negative binomial regression was inferior to binomial logistic regression, but did better at predicting if there were multiple bugs in a module
-	No concept drift was found in the data which means there wasnâ€™t significant evidence that the relation between finding bugs and the changing versions was consistent across versions.

LIMITATIONS: There is not a lot of work being done to study negative binomial regression as a valid analysis.  This article was written to put some evidence out there and to hopefully increase the research in the future.

DATA: THe data was obtained through the online public repository PROMISE.  The original data was donated by Marian Jureczko, Institute of Computer Engineering, Control and Robotics, Wroclaw University of Technology.


Allgaier, Johannes, and RÃ¼diger Pryss. "Practical approaches in evaluating validation and biases of machine learning applied to mobile health studies." Communications Medicine 4.1 (2024): 76.

GOAL: The goal of this project was to determine how good a model is by using several train-test-split methods. 

METHODS: 
-	They sorted the groups based on time to look at concept drift.  
-	They used ecological momentary assessments which test the subject in their environment and thus reduce bias while increasing validity.  
-	They use 4 points to train the model t-start, t-last which trains the model, they use t-now to test the model and t-next to predict.  
-	Stratified cross-validation (they grouped by class)
-	Nested k-fold cross validation
-	Data splitting approaches: user-cut, time-cut, user-wise, average-user

RESULTS: THey found that the ML overestimated the results because hidden groups skewed the data.  In this case it was users who gave multiple results like when they would register multiple readings for the same person.

LIMITATIONS: The main limitation they found was the hidden groups.  They were also limited because some of the groups were much larger than others because some users provided a lot of information and others didnâ€™t.  They found that their analysis worked best when the data was reviewed by experts in the field who could recognize the hidden groups that the machine couldnâ€™t

DATA: THe data came from 2 apps.  The first was Track Your Tinnitus, the second was Corona Check.

Yates, Luke A., et al. "Cross validation for model selection: a review with examples from ecology." Ecological Monographs 93.1 (2023): e1557.

GOAL: To figure out which model we should be using based on the goals of exploration, hypotheses testing, prediction, and causal inference.  We would like to have one model meet all 4 of the goals, but usually they are mutually exclusive.

METHODS: Predictive model selection is based on the estimated score for each model.  We use a formula to determine the loss.  Common scores come from established tests and are used on specific data.  For example, log likelihood goes with likelihood based estimations, squared errors goes well with Gaussian models, and etc.

RESULTS: Most instances, leave-one-out or approximate leave-one-out minimize bias.  If you canâ€™t use that because it's too expensive, they recommend a k<10.  They also recommend blocked or stratified data splitting to mitigate overfitting the data

LIMITATIONS: The major limitation to this is that in order to figure out which model you should be using, it would require you to run the method you think is the best.  The only way to know if you are right is by running different methods and comparing the score.  Another limitation is the bias-variance tradeoff.  To eliminate bias, you would have to increase the variance and vice versa.  In cross validation, you reduce bias by increasing k, but the tradeoff is not a fixed value.

DATA: Scat classification from animal feces found in California (r packet carat)and pinfish growth (r package fishmethods)


### Sai summaries

https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecm.1557Links to an external site.
https://ieeexplore.ieee.org/document/9346532Links to an external site.
Summary of the articles:
Cross validation for model selection: A review with examples from ecology
Goal:
The primary goal of the article "Cross-validation for model selection: This paper is aimed at presenting an illustrative review which stands for the importance of cross-validation in ecology with examples. Here, the authors propose several recommendations for cross-validation process to be used by ecological analysts when considering the selection of the best statistical models for their data and discuss how to transform CV into a reliable and robust tool, including methods for bias correction, the evaluation of estimation uncertainty, the choice of scores, and reducing overfitting.
Methodology:
Types of Cross-Validation:
Leave-One-Out Cross-Validation (LOO-CV): Here the data set is split into the validation and training data such that the single observation in the validation data will be used to test the model while the rest of the observations benchmark the model. The process continues with the same manner such that for each observation in the dataset, a validation data is used only once.
Exact LOO-CV: A process with complete validation, whereby, all the possible subsets of the data set are tested while leaving out only one aspect of the model.
Approximate LOO-CV: This is done with a view of reducing the computational burdens that are associated with the use of the correct LOO-CV method; influence functions or other approximations are normally used in this regard.
K-Fold Cross-Validation: Now, divide the dataset into K random partitions, this is called k-folds split, which is the basis for the k-folds technique. Here, one fold is stored in validation and the remainder (k-1) in training set or classes. For this, an iterative processing is done k times and the performance measurement of the given model is calculated through the average of results.
Split data into
ð‘˜
k subsets; train
ð‘˜
k times, each time leaving out one subset.
Large
ð‘˜
k minimizes bias and variance; bias correction needed for small
ð‘˜
k.
Approximate Leave-One-Out Cross-Validation:
Uses a weighted sum of pointwise loss values for leave-one-out score estimate.
Reduces computational cost significantly.
Blocked Cross-Validation:
Leaves out entire blocks of data to account for dependencies within data.
Bias correction required if large portions are left out.
Nested Cross-Validation:
Includes an additional inner layer of cross-validation for hyperparameter tuning.
Ensures appropriate tuning, mitigating overfitting.
Bayesian Cross-Validation:
Uses smoothing techniques to approximate predictive log likelihood.
Computed from a single set of Markov Chain Monte Carlo samples.
K-Fold with Bias Correction: Here, it is suggested that if k is small enough in the context of the current study that is k < 10 then some form of bias correction is advisable given that corrections are needed.
Bias Correction:
To minimize the bias in error estimates, especially for using k-fold cross-validation with small values of k, it is important to use bias correction techniques. Bias correction involves adjusting the validation error to account for the fact that each fold is only a part of the full dataset, thus making the error estimate more accurate.
Results:
Comparison of Cross-Validation Methods:
Illustrated two approaches to model selection using cross-validation: conditional focus and marginal focus.
Best-performing growth model differed between these focuses, highlighting the importance of selecting data-splitting schemes according to predictive focus.
Model Selection and Overfitting:
Overfitting is a significant concern, leading to spurious variables in selected models.
Calibrated selection rules are suggested to identify the simplest model with comparable predictive performance to the best-scoring model.
Technical and Conceptual Review:
Comprehensive review of cross-validation techniques addressing bias correction, estimation uncertainty, choice of predictive score, and calibrated selection rules.
Recommendations on cross-validation strategies to manage computational demands while accounting for bias and mitigating overfitting.
Predictive Scores:
Different models can be evaluated and compared based on predictive scores. These scores depend on the specific goal of modeling which may include;
Prediction Accuracy: How well a model predicts new data points.
Model Fit: How good a model describes data structure beneath.
Generalization Performance: How well a model performs on unseen data.
Commonly used predictive scores include Mean Squared Error (MSE), Mean Absolute Error (MAE), and others that measure prediction errors.
Selection Rules and Overfitting Mitigation:
One-Standard-Error Rule: In selecting the model, one commonly uses the one-standard-error option which entails picking the simplest model that is still within one standard error of the best performance. It is meant to prevent overfitting by avoiding models that are too detailed during the training phase yet lack precision during the test phase.
Modified One-Standard-Error Rule: Another version of the one-standard error rule is the adjusted one. This version fine-tunes model selection such that bias and variance are balanced.
Post-Selection Inference:
After one has chosen a model with cross-validation, one needs post-selective inferences in order to correct for the bias that is brought about by the process of selecting the models so that the future statistical inferences such as parameter estimations can be relied upon.
Limitations:
Technical Complexity:
 
Techniques for valid post-selection inference are highly technical and less accessible in didactic publications for applied analysts.
Model-Selection Uncertainty:
 
If there is model-selection uncertainty, it is crucial to reconsider making inferences on parameter estimates and instead perform model comparisons and publish the full set of results.
Computational Demands:
Managing computational demands while accounting for bias and mitigating overfitting remains challenging, despite the provided recommendations.
Data That Was Used:
The article used data from two biological case studies:
Scat Classification:
Data Source: Scat data set from the R package caret.
Details: DNA-verified species designations, fields related to the time and place of collection, and scat morphology.
Objective: Predict the biological family (felid or canid) for each scat observation.
Predictors: Eight morphological characteristics, scat location, and carbon-to-nitrogen ratio.
Pinfish Growth:
Data Source: Pinfish data from Tampa Bay from the R package fishmethods.
Details: Length, age, and sex data from 45 separate fishery hauls.
Objective: Determine which allometric growth function best describes the relationship between the length and age measurements of pinfish, accounting for the effect of sex and haul on the model parameters.
Candidate Growth Models: Gompertz, logistic, and von Bertalanffy functions
 
Citations:
Yates, Luke A., Zach Aandahl, Shane A. Richards, and Barry W. Brook. 2023. â€œ Cross Validation for Model Selection: A Review with Examples from Ecology.â€ Ecological Monographs 93(1): e1557. https://doi.org/10.1002/ecm.1557Links to an external site.
Summary 2: Super Learning with Repeated Cross Validation
This document: â€œSuper Learning with Repeated Cross Validationâ€ presents an improved version in super learning that incorporates repeated cross validation particularly in reducing variance high qualities usually presented in a single loop of cross validation. Here are the key points from the document:
Summary of Super Learning with Repeated Cross Validation
High Variance in Single Cross Validation:
While carrying out single loops of k-fold cross-validation, the variance is high, and hence the performance estimates gathered are inaccurate.
The variance of the test error estimate increases with the number of folds in cross-validation; thus, repeated Cross Validation is recommended to average the results of multiple cross Validation loops.
2.Proposed Methods
 MEAN INPUT: Takes the averages of the training data and other super learning parameters after multiple cross-validation repeats.
MEAN OUTPUT**: Averages the final results of super learning runs for at least several repeats of cross-validation. Another benefit of this method has been that it displays an enhanced performance.
1.	Computational Efficiency
Holds apply in repeated cross-validation, whereby the computational intensity is higher. To counter this, the study recommends use of 3-fold cross-validation as opposed to 10-fold due to the fact that the large training set corresponding to 90 percent of observations in 10 fold is unnecessary for accurate relative performance calibration.
2.	Experimental Results
The experiment compared the MEAN INPUT and MEAN OUTPUT on the synthetic and real datasets including biomedical data.
With a view to noting which setup showed the best performance **MEAN OUTPUT** was higher compared to **MEAN INPUT** and single cross-validation repeat.
 A few repeats seem to range around 5 per each model; nonetheless, adding more repeats can be helpful at times, especially when using the Random Forest algorithm.
3.	Practical Implications:
The document further presents the outcome of repeated 3-fold cross validation with practical similarities to the 10 fold cross validation showing that the methodology could thus be made more facile without much impact on performance.
4.	Comparative Analysis
There are some datasets, in which one algorithm is definitely superior to the rest, so, perhaps the best strategy might be to choose the one single algorithm. But when every algorithm has equal performance, computationally intensive methods such as unweighted averaging or ensemble techniques like Random Forest are greatly assisted by repeated cross-validation.
7.Data they used:
Artificial Data Sets:
 
Synthetic data created with a binary decision variable and explanatory variables drawn from a multivariate normal distribution.
Generated datasets contain 5000 explanatory variables and 200 observations.
UCI Data Sets:
Spam: 57 variables describing frequencies of words and characters in an email. The sample contains 4601 observations, with 1813 classified as spam.
Sonar: 60 variables corresponding to the strength of sonar signals reflected by a metal cylinder (111 instances) or a rock (97 instances).
Biomedical Data Sets:
ECG R-R: 184 signals of RR-intervals from 29 patients with ICD, recorded during normal heartbeat and arrhythmia. Generated 47 descriptors for analysis.
Prostate: Data from the Prostate Cancer DREAM challenge, containing 117 clinical descriptors for 923 patients, aimed at predicting survival after 180 days.
Neuroblastoma: Data from CAMDA 2017 Neuroblastoma Data Integration Challenge:
MA: 43,349 GE profiles analyzed with Agilent 44K microarrays for 145 patients.
RNA: 60,778 RNA-seq GE profiles at gene level for 145 patients.
CNV: 39,115 array CGH copy number variation profiles for 145 patients.
BRCA (Tumor/Normal) CNV: Data from the Cancer Genome Atlas with 1314 samples and 21,106 probes of copy number variation profiles.
BRCA (Endpoint): Data from the METABRIC project with disease-specific survival for 1394 patients. Includes clinical features, gene expression profiles, and copy number alterations.
These datasets were chosen to cover a range of difficulties and application areas, ensuring the robustness and versatility of the super learning approach using repeated cross-validation .
Conclusion:
The study concludes that the MEAN OUTPUT approach significantly improves predictive performance with repeated cross-validation, and that 3-fold cross-validation can effectively reduce computational complexity without compromising results.
Key Contributions:
The paper proposes a practical approach to repeated cross-validation for super learning, demonstrating its effectiveness in improving predictive accuracy.
It provides a detailed comparison of different cross-validation strategies and their impact on computational efficiency and performance.
Citation:
Mnich, A. Polewko-Klim, A. Kitlas GoliÅ„ska, W. LesiÅ„ski and W. R. Rudnicki, "Super Learning with Repeated Cross Validation," 2020 International Conference on Data Mining Workshops (ICDMW), Sorrento, Italy, 2020, pp. 629-635, doi: 10.1109/ICDMW51313.2020.00089. keywords: {Machine learning algorithms;Conferences;Prediction algorithms;Data mining;Computational complexity;Bioinformatics;Random forests;super learning;ensemble machine learning algorithms;cross validation;repeated cross validation},


Week 2 summary: Impact of the Choice of Cross-Validation Techniques on the Results of Machine Learning-Based Diagnostic Applications
Goal 
The purpose of the studyâ€ Impact of the Choice of Cross-Validation Techniques on the Results of Machine Learning-Based Diagnostic Applicationsâ€ is to assess the impact of the various cross validation techniques on performance assessment of machine learning (ML) algorithms commonly applied in diagnostics in the context of disease diagnostics. 
 Methodology 
Record-Wise Cross-Validation Techniques 
The record-wise cross validation techniques imply a division of the data set into training and validation data along with record by record but records of different subjects are not grouped together. These techniques can result in high performance estimates because the data for the amount of code and the complexity of the code can spill over from the training set into the validation set. 
Cross validation techniques used 
Stratified k-Folds Cross-Validation: 
Splits the data into 
ð‘˜ 
 folds keeping the proportion of the samples of each class the same. 
 Each fold is used once as the validation set while the other folds are used as the training set. 
 ð‘˜âˆ’1 
 Where k = n/ (kâˆ’number of folds), the kâˆ’1 folds make the training data set of the (k+1)th fold test. 
 Execution time: ~1 minute. 
 Leave-One-Out Cross-Validation (LOOCV): 
 Certification sets are applied where each record is defined as one validation set while the rest of the records make a training set. 
 Offers a nearly unbiased estimate of the model performance but it is highly time-consuming. 
 Execution time: ~Approximately 386 minutes or 6 hours 16 minutes. 
 Repeated Stratified k-Folds Cross-Validation: 
 A process that performs k-folds stratification a number of times to decrease the variability of the performance measure. 
 Execution time: ~4 minutes 49 sec. 
 Subject-Wise Cross-Validation Techniques 
 Subject-wise cross-validation techniques lead to the condition where all the records belonging to the same subject are placed in the same fold. This approach is better suited for â€˜out-of-sampleâ€™ diagnosis scenarios in which none of the subjects that are used to test the model were used during model training. 
 Stratified-Group k-Folds Cross-Validation: 
It is similar to the stratified k-fold, but it guarantees that all the samples belonging to the same subject are in the same set of the fold. 
Splits the subjects into 
 ð‘˜ 
 k groups with the aim of maintaining the proportion of samples belonging to each of the classes in question. 
 Execution time: ~ 1:10. 
 Leave-One-Group-Out Cross-Validation (LOGOCV): 
Takes each of the subjects as the validation set while the rest of the subjects constitute the training set. again, each of the subjects records is used only once in the validation set to maintain subject independence. 
 Execution time: ~/- 31 min 35 sec. 
 Repeated Stratified-Group k-Folds Cross-Validation: 
Runs the stratified-group k-folds process several times in order to obtain better results due to reduced variability. 
Execution time: ~ 4 minutes 59 seconds. 
 Comparison and Importance 
 Overfitting and Generalization: 
 Cross-validation with respect to records generally results in overfitting because the model may learn some details of that subject found in both, the training sample and the sample used for validation. This further gives a bolster estimation of the modelâ€™s performance measures such as accuracy, sensitivity, specificity, and F1-score. 
Analyte-wise methods of cross-validation give a much more accurate reflection of the modelâ€™s ability to correctly recognize new, previously unseen, subjects as they do not allow data leak and thus actually force distinct, mutually exclusive, training and validation datasets. 
Results 
 Performance Metrics: 
 depending on each subject, cross-validation gave more realistic performance measures in relation to a clinical studyâ€™s performance. 
 As far as the record-wise cross-validation is concerned, it was seen that it overestimated the performance of the classifiers as well as underestimated the classification error. 
 Classifier Performance: 
 The research established that classifiers were found to do better under record-wise cross-validation procedures because of issues to do with over-fitting and independence of subjects. 
 Limitations 
 Generalizability: 
 The authors wanted to understand the potential of using audio recordings in subjects with Parkinsonâ€™s disease and intentionally focused on this particular kind of data. 
 Although the findings may vary when they are applied to other forms of data or disease. 
 Techniques and Classifiers: 
 Barely three classifiers and restrictive types of cross-validation were examined. 
 It is also perhaps might be interesting to note that other types of classifiers or other cross validation scenarios could give even different results. 
 Dataset Size: 
 The difference between the two groups and the sample size of the dataset and its use was not indicated, and this could influence the resultsâ€™ generalizability. 
 Data Used 
Dataset: 
Audio recordings from smartphones, including consumer-grade sensors like accelerometers, gyroscopes, and microphones. 
The dataset comprised recordings from subjects with and without Parkinsonâ€™s disease. 
Conclusion 
The study concludes that subject-wise cross-validation should be preferred in diagnostic applications to provide a more accurate estimate of a modelâ€™s performance. 
Record-wise techniques, which can overestimate performance, should be avoided in scenarios that require subject independence to simulate clinical studies accurately. 
Citation: 
Tougui, Ilias & Jilbab, A. & Mhamdi, Jamal. (2021). Impact of the Choice of Cross-Validation Techniques on the Results of Machine Learning-Based Diagnostic Applications. Healthcare Informatics Research. 27. 189-199. 10.4258/hir.2021.27.3.189.
Summary 2: The Use of Cross Validation Techniques in Climate and Weather Prediction Models"
Goal
The article "The Use of Cross Validation Techniques in Climate and Weather Prediction Models" aims to explore application and effectiveness of cross-validation methods in developing and validating climate/weather forecast models. It demonstrates how these statistical techniques can enhance model accuracy and reliability by preventing overfitting and ensuring robustness across different datasets.  
K-Fold Cross-Validation:  
This method ensures that each data point goes into the validation set exactly once and into the training set kâˆ’1 times. It allows an exhaustive evaluation of the performance of a model.  
This dataset is divided into k equal parts. The model is trained on kâˆ’1 parts, and validated on the remaining part. This process is repeated k times with each part being used as the validation set once.  
Application: In climate models, k-fold cross-validation helps in evaluating how well a model can predict various weather patterns under different conditions 
Leave-One-Out Cross-Validation (LOOCV): 
 
 This is in fact, a type of k-fold cross-validation when k equals to the size of the data set. Every single iteration involves the use of a single instance for validation and the other instances for building of a model. 
Purpose: LOOCV is beneficial when the amount of data is limited because it utilizes the maximum amount of sample data during each of the iterations. This is quite stressful in terms of computation yet it gives nearly unbiased estimate of the performance of the model. 
Application: Mostly LOOCV is applied to circumstances where information is rare or would cost a lot to recognize. 
 
Bootstrapping: 
 
Several datasets are formed as a result of random sampling with replacement of the existing data. These bootstrapped samples are used for training as well as for validation of the model. 
Purpose: Cross-validation offers the sample fashioned of the model performance by including various potential training sets. Out of these, three major objectives are followed for its usage and they are: 
Application: In climate predictions, one can use bootstrapping to check the stability of the given model when assumed based on hypothetical conditions stemming from the original data set. 
Time Series Cross-Validation:
The subset selection specifically allows the training set to only contain data prior to the data in the validation set as it is utilized strictly for time series data analysis. Some of the most used methods are rolling forecasting origin and the expanding window. 
Purpose: Specifically, the technique used in this paper is called time series cross-validation, which maintains the order of the data since such an order is crucial to achieving time-based forecasts. 
Application: This method is critically relevant to weather prediction models in particular since time structure of the data needs to be preserved when conducting model evaluation. 
 
To evaluate the performance of the model during cross-validation, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used thus making the forecast more accurate and with less bias. 
 
Results: 
 
Performance Evaluation: The use of cross-validation procedures solved the problems of accuracy and reliability inherent in climate and weather prediction systems. Both procedures offered information into diverse facets of the modelâ€™s performance and offered ideas about the places that needed amendments. 
Error Metrics: Standard measures like Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) were used for comparing the error rates during cross-validation and thus make the predictions more accurate. 
Limitations: 
Computational Complexity: Computations of such techniques such as LOOCV are relatively time-consuming particularly when working with large datasets; hence not very suitable for real-time analyses. 
Data Requirements: Certain techniques, like boot strapping need relatively large amount of data to bring accurate outcomes. This can be a limitation particularly in climate studies where data gathering maybe difficult and costly. 
Temporal Dependencies: As working with temporally structured data, time series cross validation methods are required, however, across validation schemes being often intricate to execute and explain. 
Data Used: 
 
In this case, the article employed temperature, precipitation, and other climate and weather factors in their historical coefficients. The data was collected from different meteorological stations and climate data base in order to provide extensive analysis of the prediction models. 
Citation:
Michaelsen, Joel. "Cross-Validation in Statistical Climate Forecast Models." Journal of Applied Meteorology and Climatology, vol. 26, no. 11, 1987, pp. 1589-1600, doi: https://doi.org/10.1175/1520-0450(1987)026Links to an external site.<1589>2.0.CO;2.



### Curtis summaries

Week 2 Atrical Summaries

Atrical-1 [On Estimating Model Accuracy with Repeated Cross-Validation](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C10&q=On+Estimating+Model+Accuracy+with+Repeated+Cross-Validation.&btnG=) [G Vanwinckelen, H Blockeel, 2012]

Goal/why:
The authors objective is to evaluate the benefits of estimating model accuracy with Repeated Cross-Validation (CV). Repeated CV is the process of repeat the CV multiple times then calculating a mean and confidence interval around the mean. Their hypothesis is that repeated CV is not useful.

Methods:
A large data set was used to represent a population P
Nine data sets S were created by randomly sampling P n times.
- Experiment 1: The C4.5 method was used to model each S. S = 9, n = 200
- Experiment 2: The Naive Bayes method was used to model each S. S = 9, n = 200
- Experiment 3: The C4.5 method was used to model each S. S = 9, n = 1000
- Experiment 4: The Naive Bayes method was used to model each S. S = 9, n = 1000

Three 10-Fold CV estimates were calculated for each S. Each of the three CV estimates were repeated at different frequencies (1x, 10x, 30x) 
The mean CV estimate was calculated and a 95% CI was constructed around the mean. 
The population accuracy for each model was calculated.
The mean CV estimate and its upper and lower CI estimates were compared to the population accuracy.

Results:
The 95%CI range decreases as the number of CV repetitions increases. 
The model population accuracy value was within most (29/36) of the 1xCV 95%CI. As the number of CV reps. increase fewer of the model population accuracy value were within the 95%CI.
When the model population accuracy value was outside the 95%CI, most of the model population accuracy value were to the right of the 95%CI. Showing a pessimistic bias. 
The pessimistic bias decreases with an increased sample size.

Limitations:
According to the author repeated CV is recommend to reduce variance. Other CV studies have concluded that variance is greatly reduced when using the 10-fold CV, when compared to the Leave-One-Out CV method.  This experiment was done using the 10-fold CV method. I wounder if this experiment were done using LOOCV if the results/conclusions would be the same/similar.

If I  used Repetitive CV to estimate model error, I would not use the mean CV value,  I would use the max CV error estimate.

Data that was used:
Frank, A. and Asuncion, A. (2010) UCI Machine Learning Repository. University of California, School of Information and Computer Science. Irvine. http://archive.ics.uci.edu/ ml


Atrical-2 [Assessing Model Fit by Cross-Validation](https://pubs.acs.org/doi/pdf/10.1021/ci025626i?casa_token=iSpUkNhlfsMAAAAA:nzsLXNNwrbIqqpp4fR5o8nYDSRt2Jr7SCMFXFA-n0lsb7G55C3jlDrTEIeJG89MYjxlQ-nmz2Z7ScepT) [@hawkins2003assessing]

Goal:
Provide evidence that, when the sample is small ("dozens or scores rather than the hundreds"), it is better to construct a QSAR model with all available data and use Cross-validation to estimate model error. Verses constructing a QSAR model with a portion of the available data (training set) and using the remaining portion of the data (test set) to evaluate model performance. The authors refer to the later method as "sample splitting".

methods:
Data cleaning 
predictors were removing if they:

-	perfectly correlated with another predictor

-	showed no compound to compound variation

-	were available for "a few" compounds

Of the original 379 predictors, 232 were used in the experiment.

Calibration:
Training Set pool
100 of the 469 compounds were randomly selected.
300 ridge regression models were fit to the 100 selected compounds that predicted vapor pressure (y).

-	75 models were constructed with 5  randomly selected predictors (x)

-	75 models were constructed with 10 randomly selected predictors (x)

-	75 models were constructed with 20 randomly selected predictors (x) 

-	75 models were constructed with 50 randomly selected predictors (x) 

Assessment:
Of the remaining 369 (469-100 training compounds) compounds, 50 were randomly selected for the test set. 
The experimental data set was the 319 remaining compounds 

The 300 models were used to predict the vapor pressure of the compounds in the experimental data set. The R-Squared [1-(sum(yi - yhat)^2 / sum(yi - ybar)^2 )] value for each compound was calculated. The experimenters made the assumption that this R-Squared value was the "True" R-Squared value of the population. 

Four experimental R-Squared values for each of the 300 calibration models were calculated to evaluate their performance.

1. Leave One Out Cross validation

2. Hold Out Set n = 10

3. Hold Out Set n = 20

4. Hold Out Set n = 50

The "true" R-Squared value mean and standard deviation were compared to the experimental R-Squared means and standard deviations. 

Results:
With a sample size of 100:

-	The CV and 50-compond holdout set R-Squared values "seemed" to reliable predict the "true" R-Squared.

-	The 20-compond holdout R-Squared values showed more variation than  the CV and 50-compond holdout set R-Squared. The author concluded this method was inferior to CV and the 50-holdout. 

-	The 10-compund holdout R-Squared values showed "enormous" variation. The author concluded this method "useless".

limitations: 
The authors conclusions are opinion based. This statistic is smallest than this statistic. This plot looks better that this plot. The is no evidence (hypothesis testing) to support the authors conclusions. 

data that was used: 
The data set contains 469 compounds and 379 chemical descriptors. One of the descriptors was vapor pressure. Vapor pressure was the response variable in the models.

The data sets were from:

Basak, S. C.; Gute, B. D.; Grunwald, G. D. Use of topostructural
topochemical, and geometric parameters in the prediction of vapor
pressure: A hierarchical approach. J. Chem. Inf. Comput. Sci. 1997,
37, 651-655.

Basak, S. C.; Gute, B. D.; Grunwald, G. D. Use of topostructural
topochemical, and geometric parameters in the prediction of vapor
pressure: A hierarchical approach. J. Chem. Inf. Comput. Sci. 1997,
37, 651-655.

## Methods

##### Square Error

$$
Square\ Error_i = (y_i - \hat{y}_i)^2\tag{3}
$$

##### General Linear Model

$$
y = \beta_0 + \beta_1 X_1 + ... +\beta_k X_k +\varepsilon_i\tag{3}
$$

##### Mean Square Error

$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i -\hat{y}_i)^2  \tag{3}
$$


##### Leave-One_Out Cross validation

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} MSE_i  \tag{3}
$$


#### leverage statistic

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\tag{3}
$$


$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_i - \hat{y}}{1-h_i}\right)^2\tag{3}
$$



#### k-Fold Cross-Validation


$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} MSEi
$$



## Analysis and Results

### Data and Visualization

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```


### Data and Visualization

concrete data [@misc_concrete_compressive_strength_165]

| Name                          | Data Type    | Measurement  | Variable     |
|:----------------------------- |:-------------|:-------------|:----------------|
| Concrete compressive strength | Quantitative | MPa          | Response  |
| Cement              | Quantitative | kg in a m3 mixture | Predictor |
| Blast Furnace Slag  | Quantitative | kg in a m3 mixture | Predictor |
| Fly Ash             | Quantitative | kg in a m3 mixture | Predictor |
| Water               | Quantitative | kg in a m3 mixture | Predictor |
| Superplasticizer    | Quantitative | kg in a m3 mixture | Predictor |
| Coarse Aggregate    | Quantitative | kg in a m3 mixture | Predictor |
| Fine Aggregate      | Quantitative | kg in a m3 mixture | Predictor |
| Age                 | Quantitative | Day (1~365)        | Predictor |

: **Variables** {.striped .hover}

### Extract transform and Load Data

```{r, warning=FALSE, echo=TRUE}
library(dplyr)
library(readxl)

# Load Data
concrete <- read_excel("Concrete_Data.xls") %>%
  rename(cement = 1,
         furnac_slag = 2,
         fly_ash = 3,
         water = 4,
         superplasticizer = 5,
         coarse_aggregate = 6,
         fine_aggregate = 7,
         age = 8,
         compressive_strength = 9 ) %>%
  relocate(compressive_strength)

```

### Data Plots

```{r}
library(ggplot2)
library(stringr)
library(tidyr)

pivot_longer(data = concrete, 1:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x=factor(0), y = value)) +
  geom_boxplot(color="blue", outlier.color="red") +
  xlab("Predictor") + ylab("") +
  theme(axis.text.x = element_blank()) +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  theme(strip.text = element_text(face = "bold"))

pivot_longer(data = concrete, 2:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x = value, y = compressive_strength)) +
  geom_point() +
  facet_wrap(~grp, scales = "free", ncol = 3) +
  xlab("Predictor") + 
  ylab("Compressive Strength") +
  theme(strip.text = element_text(face = "bold")) +
  geom_smooth(formula = y ~ x, method = lm, se = FALSE)
```


### Statistical Modeling

### Conclusion

## References
